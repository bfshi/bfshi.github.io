<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <script type="text/javascript" src="js/js_func.js"></script>
  <title>Baifeng Shi</title>

  <meta name="author" content="Baifeng Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.jpg">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
  <td style="padding:0px">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:63%;vertical-align:middle">
        <p style="text-align:center">
          <name>Baifeng Shi</name>
        </p>
        <p>
          I am a Ph.D. student advised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> at UC Berkeley. Previously, I graduated from Peking University with a B.S. degree in computer science.
        </p>
        <p>
          I build generalist vision and robotic models.
        </p>
        <p style="text-align:center">
          <a href="mailto:baifeng_shi@berkeley.edu">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?hl=en&user=LBEIm8gAAAAJ&view_op=list_works&authuser=1&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/bfshi">Github</a> &nbsp/&nbsp
          <a href="files/cv.pdf">CV</a> &nbsp/&nbsp
          <a href="files/wechat.jpg">WeChat</a>
        </p>
      </td>
      <td style="padding:2.5%;width:35%;max-width:35%">
        <a href="images/avatar.jpg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:30px;width:100%;vertical-align:middle">
        <heading><center>Selected Publications - Vision</center></heading>

      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    
    <tr>
      <td style="padding:20px 20px 40px 20px;width:40%;vertical-align:top">
        <center><img src='images/PS3.png' width="90%"></center>
      </td>
      <td style="padding:20px;width:60%;vertical-align:top">
        <papertitle><strong>Scaling Vision Pre-Training to 4K Resolution</strong></papertitle>
        <br>
        <em>Baifeng Shi</em>,
        <a href="https://sites.google.com/site/boyilics/home">Boyi Li</a>,
        <a href="https://han-cai.github.io/">Han Cai</a>,
        <a href="https://research.nvidia.com/person/yao-lu-jason">Yao Lu</a>,
        <a href="https://sifeiliu.net/">Sifei Liu</a>,
        <a href="https://research.nvidia.com/person/marco-pavone">Marco Pavone</a>,
        <a href="https://jankautz.com/">Jan Kautz</a>,
        <a href="https://hanlab.mit.edu/songhan">Song Han</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://www.pmolchanov.com/">Pavlo Molchanov</a>,
        <a href="https://research.nvidia.com/person/danny-yin">Hongxu (Danny) Yin</a>,
        <br>
        <em>CVPR</em>, 2025
        <br>
        <red><strong>Conference highlight</strong></red>
        <br>
        <a href="javascript:toggleblock('ps3_abs')">abstract</a> /
        <a href="https://nvlabs.github.io/PS3/">website</a> /
        <a href="https://arxiv.org/pdf/2503.19903">pdf</a> /
        <a href="https://github.com/NVLabs/PS3/">code</a> /


        <p align="justify"> <i id="ps3_abs">Previous vision models such as SigLIP and DINOv2 are usually pre-trained at low resolutions (e.g., 384x384), limiting their performance of high-resolution perception. We propose PS3, a vision model that scales pre-training to 4K resolution with a near-constant cost. PS3 efficiently processes high-res images via top-down (i.e., prompt-aware) patch selection. We then introduce VILA-HD, a state-of-the-art high-res MLLM with PS3 as the vision encoder. VILA-HD achieves better performance and efficiency than previous models on various benchmarks including a 4K-res benchmark, 4KPro, that's introduced in this work. </i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px 20px 40px 20px;width:40%;vertical-align:top">
        <center><img src='images/NVILA.png' width="90%"></center>
      </td>
      <td style="padding:20px;width:60%;vertical-align:top">
        <papertitle><strong>NVILA: Efficient Frontier Visual Language Models</strong></papertitle>
        <br>
        <a href="https://zhijianliu.com/">Zhijian Liu*</a>,
        <a href="https://scholar.google.ca/citations?user=y0LVrtgAAAAJ&hl=en">Ligeng Zhu*</a>,
        <em>Baifeng Shi</em>,
        <a href="https://openreview.net/profile?id=~Zhuoyang_Zhang1">Zhuoyang Zhang</a>,
        <a href="https://github.com/NVlabs/VILA/blob/main">Yuming Lou</a>,
        <a href="https://ys-2020.github.io/">Shang Yang</a>,
        <a href="https://github.com/NVlabs/VILA/blob/main">Haocheng Xi</a>,
        <a href="https://shiyicao.com/">Shiyi Cao</a>,
        <a href="https://t1101675.github.io/">Yuxian Gu</a>,
        <a href="https://dachengli1.github.io/">Dacheng Li</a>,
        <a href="https://xiuyuli.com/">Xiuyu Li</a>,
        <a href="https://seerkfang.github.io/">Yunhao Fang</a>,
        <a href="https://yukangchen.com/">Yukang Chen</a>,
        <a href="https://chengyuhsieh.github.io/">Cheng-Yu Hsieh</a>,
        <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
        <a href="https://www.anjiecheng.me/">An-Chieh Cheng</a>,
        <a href="https://research.nvidia.com/person/vishwesh-nath">Vishwesh Nath</a>,
        <a href="https://jameshujy.github.io/">Jinyi Hu</a>,
        <a href="https://sifeiliu.net/">Sifei Liu</a>,
        <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>,
        <a href="https://research.nvidia.com/person/daguang-xu">Daguang Xu</a>,
        <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
        <a href="https://www.pmolchanov.com/">Pavlo Molchanov</a>,
        <a href="https://jankautz.com/">Jan Kautz</a>,
        <a href="https://research.nvidia.com/person/danny-yin">Hongxu (Danny) Yin^</a>,
        <a href="https://hanlab.mit.edu/songhan">Song Han^</a>,
        <a href="https://research.nvidia.com/person/yao-lu-jason">Yao Lu*^</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="javascript:toggleblock('nvila_abs')">abstract</a> /
        <a href="https://nvlabs.github.io/VILA/">website</a> /
        <a href="https://ligeng-zhu-vila.hf.space/">demo</a> /
        <a href="https://arxiv.org/pdf/2412.04468">pdf</a> /
        <a href="https://github.com/NVlabs/VILA">code</a> /
        <a href="https://huggingface.co/collections/Efficient-Large-Model/nvila-674f8163543890b35a91b428">models</a>


        <p align="justify"> <i id="nvila_abs">NVILA is a family of open VLMs designed to optimize both efficiency and accuracy for efficient video understanding and multi-image understanding . Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This "scale-then-compress" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5×, fine-tuning memory usage by 3.4×, pre-filling latency by 1.6-2.2×, and decoding latency by 1.2-2.8×. We make our code and models available to facilitate reproducibility. </i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px 20px 40px 20px;width:40%;vertical-align:top">
        <center><img src='images/s2.png' width="90%"></center>
      </td>
      <td style="padding:20px;width:60%;vertical-align:top">
        <papertitle><strong>When Do We Not Need Larger Vision Models?</strong></papertitle>
        <br>
        <em>Baifeng Shi</em>,
        <a href="https://robinwu218.github.io/">Ziyang Wu</a>,
        <a href="https://bfshi.github.io/">Maolin Mao</a>,
        <a href="https://xinw.ai/">Xin Wang</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="javascript:toggleblock('s2_abs')">abstract</a> /
        <a href="https://arxiv.org/pdf/2403.13043.pdf">pdf</a> /
        <a href="https://github.com/bfshi/scaling_on_scales">code</a> /


        <p align="justify"> <i id="s2_abs">We find that smaller vision models (e.g., ViT-B or Vit-L) run on larger image scales are usually better than larger models (e.g., ViT-H, ViT-G) and can also learn similar representations as larger models. </i></p>
        <p></p>
      </td>
    </tr>


<!--    <tr>-->
<!--      <td style="padding:20px;width:40%;vertical-align:top">-->
<!--        <center><img src='images/TOAST.png' width="90%"></center>-->
<!--      </td>-->
<!--      <td style="padding:60px 20px 20px 20px;width:60%;vertical-align:top">-->
<!--        <papertitle><strong>TOAST: Transfer Learning via Attention Steering</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://github.com/Catherine0505">Siyu Gai</a>,-->
<!--        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,-->
<!--        <a href="https://xinw.ai/">Xin Wang</a>-->
<!--        <br>-->
<!--        <em>preprint</em>, 2023-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('TOAST_abs')">abstract</a> /-->
<!--        <a href="https://arxiv.org/pdf/2305.15542.pdf">pdf</a> /-->
<!--        <a href="https://github.com/bfshi/TOAST">code</a> /-->
<!--        <a href="https://zhuanlan.zhihu.com/p/632301499">知乎</a>-->


<!--        <p align="justify"> <i id="TOAST_abs">We find that previous transfer learning methods (e.g., fine-tuning, LoRA, prompt tuning) fail to focus the model's attention on the features relevant to the downstream tasks. We show that refocusing the model's attention on task-relevant features by top-down attention can largely improve the downstream performances.</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->

    <tr>
      <td style="padding:20px;width:40%;vertical-align:top">
        <img src='images/TD-Att.png' width="100%">
      </td>
      <td style="padding:40px 20px 20px 20px;width:60%;vertical-align:top">
        <papertitle><strong>Top-Down Visual Attention from Analysis by Synthesis</strong></papertitle>
        <br>
        <em>Baifeng Shi</em>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://xinw.ai/">Xin Wang</a>
        <br>
        <em>CVPR</em>, 2023
        <br>
        <red><strong>Conference highlight</strong></red>
        <br>
        <a href="https://sites.google.com/view/absvit">website</a> /
        <a href="javascript:toggleblock('AbSViT_abs')">abstract</a> /
        <a href="https://arxiv.org/pdf/2303.13043.pdf">pdf</a> /
        <a href="https://github.com/bfshi/AbSViT">code</a> /
        <a href="https://zhuanlan.zhihu.com/p/623354740">知乎</a>


        <p align="justify"> <i id="AbSViT_abs">We build ViTs with the ability of top-down attention, i.e., steering its attention to specific objects when given a prompt.</i></p>
        <p></p>
      </td>
    </tr>

<!--    <tr>-->
<!--      <td style="padding:20px;width:40%;vertical-align:middle">-->
<!--        <img src='images/VARS.png' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Visual Attention Emerges from Recurrent Sparse Reconstruction</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="http://people.csail.mit.edu/yalesong/home/">Yale Song</a>,-->
<!--        <a href="https://neelj.com/">Neel Joshi</a>,-->
<!--        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,-->
<!--        <a href="https://xinw.ai/">Xin Wang</a>-->
<!--        <br>-->
<!--        <em>ICML</em>, 2022-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('VARS_abs')">abstract</a> /-->
<!--        <a href="https://arxiv.org/abs/2204.10962">pdf</a> /-->
<!--        <a href="https://github.com/bfshi/VARS">code</a>-->

<!--        <p align="justify"> <i id="VARS_abs">Visual attention helps achieve robust perception under noise, corruption, and distribution shifts in human vision, which are areas where modern neural networks still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity. Related features are grouped together via recurrent connections between neurons, with salient objects emerging via sparse regularization. VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time. Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of "templates" encoding underlying patterns of data. We show that self-attention is a special case of VARS with a single-step optimization and no sparsity constraint. VARS can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks.</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--      <td style="padding:10px;width:40%;vertical-align:middle">-->
<!--        <img src='images/ICCV2021_SSAD_OSAD.jpg' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Temporal Action Detection with Multi-level Supervision</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,-->
<!--        <a href="https://www.cc.gatech.edu/~judy/">Judy Hoffman</a>,-->
<!--        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,-->
<!--        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,-->
<!--        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>-->
<!--        <br>-->
<!--        <em>ICCV</em>, 2021-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('ICCV2021_SSAD_OSAD_abs')">abstract</a> /-->
<!--        <a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Temporal_Action_Detection_With_Multi-Level_Supervision_ICCV_2021_paper.pdf">pdf</a>-->

<!--        <p align="justify"> <i id="ICCV2021_SSAD_OSAD_abs">Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification literature. Identifying that the main source of error is action incompleteness (ie, missing parts of actions), we alleviate it by designing an unsupervised foreground attention (UFA) module utilizing the conditional independence between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. To overcome the accompanying action-context confusion problem in OSAD baselines, an information bottleneck (IB) is designed to suppress the scene information in non-action frames while preserving the action information. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->


<!--    <tr>-->
<!--      <td style="padding:10px;width:40%;vertical-align:middle">-->
<!--        <img src='images/NeurIPS2020_ARML.png' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Auxiliary Task Reweighting for Minimum-data Learning</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://www.cc.gatech.edu/~judy/">Judy Hoffman</a>,-->
<!--        <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,-->
<!--        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,-->
<!--        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>-->
<!--        <br>-->
<!--        <em>NeurIPS</em>, 2020-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('NeurIPS2020_ARML_abs')">abstract</a> /-->
<!--        <a href="https://arxiv.org/abs/2010.08244">pdf</a> /-->
<!--        <a href="https://github.com/bfshi/ARML_Auxiliary_Task_Reweighting">code</a> /-->
<!--        <a href="https://sites.google.com/view/auxiliary-task-reweighting/home">project page</a> /-->
<!--        <a href="https://drive.google.com/file/d/1UiNHBztUVJbhvDJTmFww7v4ZPSdXD4DM/view">slides</a> /-->
<!--        <a href="https://www.youtube.com/watch?v=B2i6z6HefGw">video</a>-->

<!--        <p align="justify"> <i id="NeurIPS2020_ARML_abs">Auxiliary tasks are widely used to address the lack of data by providing additional supervision in semi/self-supervised learning, transfer learning, reinforcement learning, etc. Assigning the importance weights for different auxiliary tasks remains a crucial, and largely understudied, research question. In this work, we formulate the problem as minimizing the information required to learn the main task. With the key insight that information required for inference can be reduced by a good prior, we propose an algorithm to automatically reweight auxiliary tasks so that the surrogate prior given by the weighted likelihood of auxiliary tasks is optimized. We further reduce the optimization problem into minimizing the l2 distance between main/auxliary task gradients by adopting tools including Langevin dynamics, Fisher divergence, etc. Our algorithm finds the optimal weights and minimizes the required data under various settings, as supported by both theoretical guarantees and experimental observations.</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--      <td style="padding:10px;width:40%;vertical-align:middle">-->
<!--        <img src='images/ECCV2020.png' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Weakly-supervised Action Localization with Expectation-Maximization Multi-instance Learning</strong></papertitle>-->
<!--        <br>-->

<!--        <a href="https://scholar.google.com/citations?user=kqVZxpYAAAAJ&hl=en">Zhekun Luo</a>,-->
<!--        <a href="https://www.devinguillory.com/">Devin Guillory</a>,-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://scholar.google.com/citations?hl=zh-CN&user=BENt-uEAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate">Wei Ke</a>,-->
<!--        <a href="https://scholar.google.com/citations?hl=zh-CN&user=0IKavloAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate">Fang Wan</a>,-->
<!--        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,-->
<!--        <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>-->
<!--        <br>-->
<!--        <em>ECCV</em>, 2020-->
<!--        <br>-->
<!--        <a href="https://arxiv.org/abs/2004.00163">pdf</a>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--      <td style="padding:10px;width:40%;vertical-align:middle">-->
<!--        <img src='images/ICML2020_InfoDrop.jpg' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Informative Dropout for Robust Representation Learning: A Shape-bias Perspective</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://zdhnarsil.github.io/">Dinghuai Zhang*</a>,-->
<!--        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,-->
<!--        <a href="https://scholar.google.co.uk/citations?user=a2sHceIAAAAJ&hl=en">Zhanxing Zhu</a>,-->
<!--        <a href="http://www.muyadong.com/index.html">Yadong Mu</a>,-->
<!--        <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,-->
<!--        <br>-->
<!--        <em>ICML</em>, 2020-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('ICML2020_InfoDrop_abs')">abstract</a> /-->
<!--        <a href="https://arxiv.org/abs/2008.04254">pdf</a> /-->
<!--        <a href="https://github.com/bfshi/InfoDrop">code</a> /-->
<!--        <a href="https://icml.cc/virtual/2020/poster/5839">video</a> /-->
<!--        <a href="https://zhuanlan.zhihu.com/p/197929813">知乎</a>-->

<!--        <p align="justify"> <i id="ICML2020_InfoDrop_abs">In this work, we attempt at improving various kinds of robustness universally by alleviating CNN’s texture bias. With inspiration from the human visual system, we propose to discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. We observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation).</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--      <td style="padding:10px;width:40%;vertical-align:middle">-->
<!--        <img src='images/CVPR2020_DGAM.png' width="100%">-->
<!--      </td>-->
<!--      <td style="padding:20px;width:60%;vertical-align:middle">-->
<!--        <papertitle><strong>Weakly-supervised Action Localization by Generative Attention Modeling</strong></papertitle>-->
<!--        <br>-->
<!--        <em>Baifeng Shi</em>,-->
<!--        <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a>,-->
<!--        <a href="http://www.muyadong.com/index.html">Yadong Mu</a>,-->
<!--        <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,-->
<!--        <br>-->
<!--        <em>CVPR</em>, 2020-->
<!--        <br>-->
<!--        <a href="javascript:toggleblock('CVPR2020_DGAM_abs')">abstract</a> /-->
<!--        <a href="http://arxiv.org/abs/2003.12424">pdf</a> /-->
<!--        <a href="https://github.com/bfshi/DGAM-Weakly-Supervised-Action-Localization">code</a> /-->

<!--        <p align="justify"> <i id="CVPR2020_DGAM_abs">Regular weakly-supervised action localization methods mostly follows the “localizing by classifying” paradigm, which results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves. With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model is learned to model the class-agnostic likelihood of each frame given the attention. By maximizing the conditional probability, we obtain the MAP estimation of the attention.</i></p>-->
<!--        <p></p>-->
<!--      </td>-->
<!--    </tr>-->


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:30px;width:100%;vertical-align:middle">
        <heading><center>Selected Publications - Robotics</center></heading>

      </td>
    </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px 20px 100px 20px;width:40%;vertical-align:top">
        <center><img src='images/humanoid_next_token_prediction.gif' width="90%"></center>
      </td>
      <td style="padding:20px;width:60%;vertical-align:top">
        <papertitle><strong>Humanoid Locomotion as Next Token Prediction</strong></papertitle>
        <br>
        <a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a>,
        <a href="https://bikezhang106.github.io/">Bike Zhang</a>,
        <em>Baifeng Shi</em>,
        <a href="https://people.eecs.berkeley.edu/~jathushan/">Jathushan Rajasegaran</a>,
        <a href="https://humanoid-next-token-prediction.github.io/">Sarthak Kamat</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
        <a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath</a>,
        <a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <red><strong>Spotlight</strong></red>
        <br>
        <a href="javascript:toggleblock('humanoid_next_token_prediction_abs')">abstract</a> /
        <a href="https://arxiv.org/pdf/2402.19469.pdf">pdf</a> /
        <a href="https://humanoid-next-token-prediction.github.io/">website</a> /


        <p align="justify"> <i id="humanoid_next_token_prediction_abs">We formulate humanoid locomotion as a next token prediction problem. This enables learning to walk from in-the-wild data such as Youtube videos.</i></p>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px 20px 60px 20px;width:40%;vertical-align:top">
        <center><img src='images/RPT.gif' width="90%"></center>
      </td>
      <td style="padding:20px;width:60%;vertical-align:top">
        <papertitle><strong>Robot Learning with Sensorimotor Pre-training</strong></papertitle>
        <br>
        <a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a>,
        <em>Baifeng Shi</em>,
        <a href="https://max-fu.github.io/">Letian Fu</a>,
        <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a>,
        <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a>,
        <a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik*</a>
        <br>
        <em>CoRL</em>, 2023
        <br>
        <red><strong>Oral Presentation</strong></red>
        <br>
        <a href="javascript:toggleblock('RPT_abs')">abstract</a> /
        <a href="https://arxiv.org/pdf/2306.10007.pdf">pdf</a> /
        <a href="https://robotic-pretrained-transformer.github.io/">website</a> /


        <p align="justify"> <i id="RPT_abs">We make imitation learning easier by MAE pre-training on sensorimotor sequences.</i></p>
        <p></p>
      </td>
    </tr>


    </tbody></table>

<!--    <table class="center"><tbody>-->
<!--    <tr>-->
<!--      <td>-->
<!--        <heading>Experience</heading>-->
<!--      </td>-->
<!--    </tr>-->
<!--    </tbody></table>-->
<!--    <table width="50%" align="center" border="0" cellpadding="10"><tbody>-->
<!--    <tr>-->
<!--      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle"><img src="images/ucb.png", width="90%"></td>-->
<!--      <td width="80%" valign="center" >-->
<!--        <b>University of California, Berkeley</b>-->
<!--        <br> 2021.09 - Present-->
<!--        <br>-->
<!--        <br> <b>Ph.D. Student in Computer Science</b>-->
<!--        <br> Advisor: Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>-->
<!--      </td>-->
<!--    </tr>-->
<!--    <tr>-->
<!--      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle;"><img src="images/microsoft.png", width="90%"></td>-->
<!--      <td width="80%" valign="center">-->
<!--        <b>Microsoft Research</b>-->
<!--        <br> 2021.09 - 2022.02-->
<!--        <br>-->
<!--        <br> <b>Research Collaboration via BAIR Commons Program</b>-->
<!--        <br> Mentor: Dr. <a href="https://xinw.ai">Xin Wang</a>-->
<!--      </td>-->
<!--    </tr>-->
<!--    <tr>-->
<!--      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle;"><img src="images/microsoft.png", width="90%"></td>-->
<!--      <td width="80%" valign="center">-->
<!--        <b>Microsoft Research Asia</b>-->
<!--        <br> 2019.09 - 2021.02-->
<!--        <br>-->
<!--        <br> <b>Research Intern</b>-->
<!--        <br> Mentor: Dr. <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en">Qi Dai</a> and Dr. <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>-->
<!--      </td>-->
<!--    </tr>-->
<!--    <tr>-->
<!--      <td style="padding-left:20px;padding-right:20px;width:35%;vertical-align:middle"><img src="images/peking_university.png", width="90%"></td>-->
<!--      <td width="80%" valign="center">-->
<!--        <b>Peking University</b>-->
<!--        <br> 2017.09 - 2021.06-->
<!--        <br>-->
<!--        <br> <b>B.S. in Computer Science</b>, Turing Class-->
<!--        <br> Advisor: Prof. <a href="http://www.muyadong.com/index.html">Yadong Mu</a>-->
<!--      </td>-->
<!--    </tr>-->


<!--    </tbody></table>-->



    <!-- Invited Talks Section -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading><center>Invited Talks</center></heading>
        </td>
      </tr>
      </tbody>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:top;">
          <p>[Apr 2025] &nbsp; <strong>Scaling Vision Pre-Training to 4K Resolution</strong>, Boston University, hosted by <a href="https://cs-people.bu.edu/tianle/">Tianle Chen</a> and <a href="https://bryanplummer.com/">Bryan Plummer</a> &nbsp; [<a href="https://drive.google.com/file/d/15Cwo_Mpjl1IkZLAhLXsCxrwkcCcFhf9z/view?usp=sharing">slides</a>]</p>
          <p>[Apr 2025] &nbsp; <strong>Scaling Vision Pre-Training to 4K Resolution</strong>, Princeton University, hosted by <a href="https://xindiwu.github.io/">Xindi Wu</a>, <a href="https://tylerzhu.com/">Tyler Zhu</a>, and <a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> &nbsp; [<a href="https://drive.google.com/file/d/15Cwo_Mpjl1IkZLAhLXsCxrwkcCcFhf9z/view?usp=sharing">slides</a>]</p>
          <p>[Apr 2025] &nbsp; <strong>Scaling Vision Pre-Training to 4K Resolution</strong>, Google Deepmind, hosted by <a href="https://tengdahan.github.io/">Tengda Han</a></p>
          <p>[Jun 2024] &nbsp; <strong>Scaling Up Visual Pre-Training: What’s Next?</strong>, AI Tea Talk Singapore, hosted by <a href="https://kaiwang960112.github.io/">Kai Wang</a></p>
          <p>[Apr 2024] &nbsp; <strong>Scaling Up Visual Pre-Training: What’s Next?</strong>, VGG @ University of Oxford, hosted by <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a> &nbsp; [<a href="https://docs.google.com/presentation/d/1hGlRaeX3B7VYu7DAy3NwAZYlY9g8Jn_Q/edit?usp=sharing&ouid=107067608144964181492&rtpof=true&sd=true">slides</a>]</p>
          <p>[Mar 2024] &nbsp; <strong>Scaling Up Visual Pre-Training: What’s Next?</strong>, Prof. Yi Ma's group @ UC Berkeley, hosted by <a href="https://robinwu218.github.io/">Ziyang Wu</a></p>
          <p>[Oct 2023] &nbsp; <strong>Principles and Applications of Bottom-Up and Top-Down Visual Attention</strong>, Peking University, hosted by <a href="https://selina2023.github.io/">Yufei Ding</a> &nbsp; [<a href="https://docs.google.com/presentation/d/1ewlm-WKh5xhmtalOvVYJve-Buwz-YHnO/edit?usp=sharing&ouid=107067608144964181492&rtpof=true&sd=true">slides</a>]</p>
          <p>[Jun 2023] &nbsp; <strong>Principles and Applications of Bottom-Up and Top-Down Visual Attention</strong>, TechBeat</p>
        </td>
      </tr>
      </tbody>
    </table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
<!--        <p style="text-align:right;font-size:small;">-->
<!--          <br> Last updated: Mar 18, 2024-->
<!--        </p>-->
      </td>
    </tr>
    </tbody></table>
  </td>
</tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideblock('ps3_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nvila_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('s2_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('humanoid_next_token_prediction_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('RPT_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('TOAST_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('AbSViT_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('VARS_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ICCV2021_SSAD_OSAD_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('NeurIPS2020_ARML_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ICML2020_InfoDrop_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('CVPR2020_DGAM_abs');
</script>

</body>

</html>
